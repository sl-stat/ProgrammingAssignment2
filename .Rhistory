train.ErrP = sum(abs(train.YhatP - Ytrain))/length(Ytrain)
train.err.storeP = c(train.err.storeP, train.ErrP)
test.YhatP = predict(bestpmod, newdata = test)
test.ErrP = sum(abs(test.YhatP - Ytest))/length(Ytest)
test.err.storeP = c(test.err.storeP, test.ErrP)
}
train.err.storeP
test.err.storeP
x11()
upper = max(train.err.storeP, test.err.storeP)
lower = min(train.err.storeP, test.err.storeP)
plot(train.err.storeP, type = "o", lty = 2, col = "green", xlab = "Cost Parameter", xaxt = "n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Misclassification Rate", main = "Polynomial Kernel, Error of Different Cost Parameters")
lines(test.err.storeP, type = "o", lty = 1, col = "orange")
axis(1, at=1:4, lab=c("0.01", "1", "5", "10"))
legend("topright", c("training","test"), lty = c(2,1), col = c("green","orange"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
plot(training_errors, type = "o", col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "o", col = "red")
legend("topright", c("training","test"), col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
plot(training_errors, type = "o", col = "green", xlab = "Kernel Type", xaxt="n", ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "o", col = "red")
legend("topright", c("training","test"), col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, type = "o", col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "o", col = "red")
legend("topright", c("training","test"), col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, type = "o", col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "o", col = "red")
legend("topleft", c("training","test"), col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, type = "p", col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), type = "p",  col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, type = "p", col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
?legend
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, type = "p", col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), lty = 1, col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, type = "p", col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), pch = 1, col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
#####################################
#  Shuo Li's STA545 HW5 Q3
#  Dec.4, 2014
#####################################
rm(list=ls())
setwd("D:\\StudyandWork\\Study\\0School\\14 Fall\\STA 545\\hw5")
#install.packages("ISLR")
library(ISLR)                 # has data
#install.packages("e1071")
library(e1071)                # has SVM
data(OJ)
dim(OJ)    # 1070   18
head(OJ)
OJ[,1] = as.numeric(OJ$Purchase)-1     # 0:CH  1:MM
head(OJ)
set.seed(2350)
train_i = sample(nrow(OJ), 0.8*nrow(OJ))
training = OJ[train_i,]
Ytrain = training[,1]
test = OJ[-train_i,]
Ytest = test[,1]
### (A) SVM with linear kernel with varying cost parameters ###
#lmodel = tune(svm, Purchase~., data = training, kernel = "linear", ranges = list(cost = c(0.01, 1, 5, 10)))
#lmodel   # - best parameters: cost 0.01   - best performance: 0.1270745
#summary(lmodel)
#- Detailed performance results:
#  cost     error dispersion
#1  0.01 0.1270745 0.01760092
#2  1.00 0.1287323 0.01878164
#3  5.00 0.1288514 0.01866084
#4 10.00 0.1288328 0.01860805
## However, since I need to use prediction to check error to select the optimal, I need to do the above in a loop ##
train_err_store = c()
test_err_store = c()
cost = c(0.01, 1, 5, 10)
for (i in 1:length(cost)){
lmodel = tune(svm, Purchase ~ ., data = training, kernel = "linear", ranges = list(cost = cost[i]))
bestlmod = lmodel$best.model
trainY_output = predict(bestlmod, newdata = training)
trainYhat = round(trainY_output)
trainErr = sum(abs(trainYhat - Ytrain))/length(Ytrain)
train_err_store = c(train_err_store, trainErr)
testY_output = predict(bestlmod, newdata = test)
testYhat = round(testY_output)
testErr = sum(abs(testYhat - Ytest))/length(Ytest)
test_err_store = c(test_err_store, testErr)
}
train_err_store  #0.1612149533 0.1588785047 0.1600467290 0.1612149533
test_err_store   #0.2149532710 0.2196261682 0.2196261682 0.2196261682
## plot the errors for linear kernel SVM ##
x11()
upper = max(train_err_store, test_err_store)
lower = min(train_err_store, test_err_store)
plot(train_err_store, type = "o", lty = 2, col = "blue", xlab = "Cost Parameter", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Misclassification Rate", main = "Linear Kernel, Error of Different Cost Parameters")
lines(test_err_store, type = "o", lty = 1, col = "red")
axis(1, at=1:4, lab=c("0.01", "1", "5", "10"))
legend("topright", c("training","test"), lty = c(2,1), col = c("blue","red"))
train.err.store = c()
test.err.store = c()
cost = c(0.01, 1, 5, 10)
for (i in 1:length(cost)){
rmodel = tune(svm, Purchase~., data = training, kernel = "radial", ranges = list(cost = cost[i]))
bestrmod = rmodel$best.model
train.Yhat = predict(bestrmod, newdata = training)
train.Err = sum(abs(train.Yhat - Ytrain))/length(Ytrain)
train.err.store = c(train.err.store, train.Err)
test.Yhat = predict(bestrmod, newdata = test)
test.Err = sum(abs(test.Yhat - Ytest))/length(Ytest)
test.err.store = c(test.err.store, test.Err)
}
train.err.store  #0.3687645444 0.2028145809 0.1861116572 0.1817623818
test.err.store   #0.3661747316 0.2671403177 0.2672105036 0.2680293297
## plot the errors for radial kernel SVM ##
x11()
upper = max(train.err.store, test.err.store)
lower = min(train.err.store, test.err.store)
plot(train.err.store, type = "o", lty = 2, col = "green", xlab = "Cost Parameter", xaxt = "n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Misclassification Rate", main = "Radial Kernel, Error of Different Cost Parameters")
lines(test.err.store, type = "o", lty = 1, col = "orange")
axis(1, at=1:4, lab=c("0.01", "1", "5", "10"))
legend("topright", c("training","test"), lty = c(2,1), col = c("green","orange"))
train.err.storeP = c()
test.err.storeP = c()
cost = c(0.01, 1, 5, 10)
for (i in 1:length(cost)){
pmodel = tune(svm, Purchase~., data = training, kernel = "polynomial", degree = 2, ranges = list(cost = cost[i]))
bestpmod = pmodel$best.model
train.YhatP = predict(bestpmod, newdata = training)
train.ErrP = sum(abs(train.YhatP - Ytrain))/length(Ytrain)
train.err.storeP = c(train.err.storeP, train.ErrP)
test.YhatP = predict(bestpmod, newdata = test)
test.ErrP = sum(abs(test.YhatP - Ytest))/length(Ytest)
test.err.storeP = c(test.err.storeP, test.ErrP)
}
train.err.storeP  # 0.3767361484 0.2330471304 0.2261182197 0.2251866527
test.err.storeP   # 0.3714969098 0.2830652858 0.2789910962 0.2792804346
## plot the errors for polynomial kernel SVM ##
x11()
upper = max(train.err.storeP, test.err.storeP)
lower = min(train.err.storeP, test.err.storeP)
plot(train.err.storeP, type = "o", lty = 2, col = "green", xlab = "Cost Parameter", xaxt = "n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Misclassification Rate", main = "Polynomial Kernel, Error of Different Cost Parameters")
lines(test.err.storeP, type = "o", lty = 1, col = "orange")
axis(1, at=1:4, lab=c("0.01", "1", "5", "10"))
legend("topright", c("training","test"), lty = c(2,1), col = c("green","orange"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, type = "p", col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), pch = 1, col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, pch = 19, col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), pch = 19, col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, pch = 19, col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, pch = 19, col = "red")
legend("topleft", c("training","test"), pch = 19, col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, pch = 19, col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), pch = 19, col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, pch = 19, col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), pch = c(19,1), col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
x11()
training_errors = c(min(train_err_store), min(train.err.store), min(train.err.storeP))
test_errors = c(min(test_err_store), min(test.err.store), min(test.err.storeP))
lower = min(training_errors, test_errors)
upper = max(training_errors, test_errors)
plot(training_errors, pch = 19, col = "green", xlab = "Kernel Type", xaxt="n", ylim = c(lower - 0.01, upper + 0.01), ylab = "Lowest Misclassification Rate", main = "SVM, Error of Different Cost Parameters")
lines(test_errors, type = "p", col = "red")
legend("topleft", c("training","test"), pch = c(19,1), col = c("green","red"))
axis(1, at=1:3, lab=c("Linear", "Radial", "Polynomial"))
install.packages("KernSmooth")
library(KernSmooth)
find.packages("devtools")
install.packages("devtools")
library(devtools)
find_rtools()
rm(list=ls())
library(mlbench)
library(mlbench)
install.packages("mlbench")
library(mlbench)
data(BreastCancer)
dim(BreastCancer)
head(BreastCancer)
getwd()
q()
y <-data.frame(a=1, b="a")
deput(y)
dput(y)
dput(y, file = "y.R")
new.y<-dget("y.R")
new.y
x <- c(4, TRUE)
class(x)
x
x <- list(2, "a", "b", TRUE)
x[[1]]
x[[2]]
x <- c(3, 5, 1, 10, 12, 6)
x[x %in% 1:5] <- 0
x
x <- c(4, "a", TRUE)
class(x)
x <- list(2, "a", "b", TRUE)
x[[1]]
length(x[[1]])
class(x[[1]])
x <- 1:4
y <- 2:3
x+y
class(x+y)
class(x)
rm(list=ls())
x <- 1:4
y <- 2:3
class(x+y)
?numeric
cube <- function(x, n) {
x^3
}
cube(3)
x <- 1:10
if(x > 5) {
x <- 0
}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
y
g
}
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
y
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
rm(list=ls())
source("http://d396qusza40orc.cloudfront.net/rprog%2Fscripts%2Fsubmitscript1.R")
submit()
printmessage = function(x){}
printmessage = function(x){
if(x>0)
print("x is greater than zero")
else
print("x is less than or equal to zero")
}
printmessage(3)
printmessage(-4)
printmessage = function(x){
if(x>0)
print("x is greater than zero")
else
print("x is less than or equal to zero")
invisible(x)
}
printmessage(3)
printmessage(-4)
?invisible
library(datasets)
data(iris)
head(iris)
?iris
?sapply
?tapply
x = iris$Sepal.Length
mean = tapply(x,iris$Species = 'virginica', mean)
x = iris$Sepal.Length
mean = tapply(x,Species = 'virginica', mean)
xbar = tapply(x,Species = 'virginica', mean)
x = iris$Sepal.Length
xbar = tapply(x,Species = 'virginica', mean, simplify = FALSE)
x = iris$Sepal.Length
factor = iris$virginica
xbar = tapply(x,factor, mean, simplify = FALSE)
x = iris$Sepal.Length
factor = iris$Species
xbar = tapply(x,factor=virginica, mean)
x = iris$Sepal.Length
factor = iris$Species
xbar = tapply(x,factor="virginica", mean)
class(x)
as.vector(x)
x = as.vector(x)
xbar = tapply(x,factor="virginica", mean)
xbar = tapply(x,factor="virginica", mean, simplify = F)
xbar = tapply(x,factor="virginica", FUN = mean, simplify = F)
xbar = tapply(x,factor, FUN = mean, simplify = F)
xbar
class(x)   #"numeric"
x = as.vector(x)
xbar = tapply(x,factor, mean, simplify = F)
xbar
x = iris$Sepal.Length
factor = iris$Species
class(x)   #"numeric"
# x = as.vector(x)
xbar = tapply(x,factor, mean)
xbar
x = iris$Sepal.Length
factor = iris$Species
xbar = tapply(x,factor, mean)
xbar
head(iris)
colMeans(iris)
apply(iris, 1, mean)
apply(iris[, 1:4], 2, mean)
?apply
library(datasets)
data(mtcars)
?mtcars
head(mecars)
head(mtcars)
mpg <- tapply(mtcars$mpg, mtcars$cyl, mean)
mpg
?sapply
apply(mtcars, 2, mean)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
mpg4 <- tapply(mtcars$mpg, mtcars$cyl=4, mean)
tapply(mtcars$mpg, mtcars$cyl, mean)
mpg <- tapply(mtcars$mpg, mtcars$cyl, mean)
mpg
mpg[1]-mpg[3]
diff <- abs(mpg[1]-mpg[3])
diff
class(diff)
debug(ls)
ls
ls()
3
4
data(mtcars)
?mtcars
head(mtcars)
#Q3: How can one calculate the average miles per gallon (mpg)
#    by number of cylinders in the car (cyl)?
undebug(ls)
x = iris$Sepal.Length
factor = iris$Species
xbar = tapply(x,factor, mean)
xbar
hp <- tapply(mtcars$horsepower, mtcars$cyl, mean)
head(mtcars)
hp <- tapply(mtcars$hp, mtcars$cyl, mean)
diff <- abs(hp[1]-hp[3])
diff
hp
makeVector <- function(x = numeric()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setmean <- function(mean) m <<- mean
getmean <- function() m
list(set = set, get = get,
setmean = setmean,
getmean = getmean)
}
?<<-
get
x
x=1,4,7,9
x = c(1,4,7,9)
makeVector(x)
m
makeVector <- function(x = numeric()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setmean <- function(mean) m <<- mean
getmean <- function() m
list(set = set, get = get,
setmean = setmean,
getmean = getmean)
}
> x <- makeVector(c(1,2,3))
x <- makeVector(c(1,2,3))
x$get()
# Test your code
source("draft, should delete later.R")
source("draft.R")
# Test your code
source("draft.R")
#
# generate matrix, and the inverse of the matrix.
size <- 100 # size of the matrix edge, don't make this too big
mymatrix <- matrix(rnorm(size^2), nrow=size, ncol=size)
mymatrix.inverse <- solve(mymatrix)
#
# now solve the matrix via the cache-method
#
special.matrix   <- makeCacheMatrix(mymatrix)
#
# this should take long, since it's the first go
special.solved.1 <- cacheSolve(special.matrix)
#
# this should be lightning fast
special.solved.2 <- cacheSolve(special.matrix)
#
# check if all solved matrices are identical
identical(mymatrix.inverse, special.solved.1) & identical(mymatrix.inverse, special.solved.2)
#
# should return TRUE
getwd
getwd()
setwd("C:\Users\Star.Li\Desktop\Rprogramming\Assignment_2\ProgrammingAssignment2")
setwd("C:\\Users\\Star.Li\\Desktop\\Rprogramming\\Assignment_2\\ProgrammingAssignment2")
getwd()
# Test your code
source("draft.R")
# generate matrix, and the inverse of the matrix.
size <- 100 # size of the matrix edge, don't make this too big
mymatrix <- matrix(rnorm(size^2), nrow=size, ncol=size)
mymatrix.inverse <- solve(mymatrix)
special.matrix   <- makeCacheMatrix(mymatrix)
special.solved.1 <- cacheSolve(special.matrix)
special.solved.2 <- cacheSolve(special.matrix)
# check if all solved matrices are identical
identical(mymatrix.inverse, special.solved.1) & identical(mymatrix.inverse, special.solved.2)
#
